{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xjdr-alt/entropix/blob/main/entropix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xZd9EBLmgKU6"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from typing import Dict, List, NamedTuple, Optional, Tuple\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# <thought>\n",
        "# The use of JAX instead of PyTorch or TensorFlow is interesting. This suggests\n",
        "# a focus on high-performance computing, possibly leveraging JAX's JIT compilation\n",
        "# and automatic differentiation capabilities. However, it also means the code\n",
        "# might be less accessible to developers more familiar with PyTorch or TensorFlow.\n",
        "# </thought>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v7mNYwHnPJz"
      },
      "source": [
        "# Set Model ID and Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oHaZYRqAnRPS"
      },
      "outputs": [],
      "source": [
        "# Set Model ID and Token\n",
        "MODEL_ID = 'meta-llama/Llama-3.2-1B-Instruct'\n",
        "TOKEN = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2IdvdlSltd9"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RHxP8Bd1lvo8"
      },
      "outputs": [],
      "source": [
        "# Config\n",
        "params = {\n",
        "  \"dim\": 2048,\n",
        "  \"n_layers\": 16,\n",
        "  \"n_heads\": 32,\n",
        "  \"n_kv_heads\": 8,\n",
        "  \"vocab_size\": 128256,\n",
        "  \"ffn_dim_multiplier\": 1.5,\n",
        "  \"multiple_of\": 256,\n",
        "  \"norm_eps\": 1e-05,\n",
        "  \"rope_theta\": 500000.0,\n",
        "  \"use_scaled_rope\": True,\n",
        "  \"max_seq_len\": 4096\n",
        "}\n",
        "\n",
        "class ModelParams(NamedTuple):\n",
        "  n_layers: int\n",
        "  n_local_heads: int\n",
        "  n_local_kv_heads: int\n",
        "  head_dim: int\n",
        "  max_seq_len: int\n",
        "  rope_theta: float\n",
        "  use_scaled_rope: bool\n",
        "\n",
        "LLAMA_1B_PARAMS = ModelParams(\n",
        "  n_layers=params[\"n_layers\"],\n",
        "  n_local_heads=params[\"n_heads\"],\n",
        "  n_local_kv_heads=params[\"n_kv_heads\"],\n",
        "  head_dim=params[\"dim\"] // params[\"n_heads\"],\n",
        "  max_seq_len=params[\"max_seq_len\"],\n",
        "  rope_theta=params[\"rope_theta\"],\n",
        "  use_scaled_rope=params[\"use_scaled_rope\"]\n",
        ")\n",
        "\n",
        "# <thought>\n",
        "# The use of a NamedTuple for model parameters is a good practice for type safety\n",
        "# and readability. However, some of these parameters (like rope_theta) are quite\n",
        "# specific and might benefit from more documentation about their purpose and impact.\n",
        "# \n",
        "# The rope_theta value (500000.0) is much larger than the typical value (10000.0)\n",
        "# used in many transformer models. This could significantly affect the model's\n",
        "# ability to handle long sequences and might be worth investigating further.\n",
        "# </thought>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0fWAb83ghC1"
      },
      "source": [
        "# Download Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHcDsZzshQji"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "flDeKnQlhS1J"
      },
      "outputs": [],
      "source": [
        "# Download Weights\n",
        "import os\n",
        "import torch\n",
        "import ml_dtypes\n",
        "from pathlib import Path\n",
        "from transformers import AutoModelForCausalLM\n",
        "from unittest.mock import patch\n",
        "from transformers.dynamic_module_utils import get_imports\n",
        "\n",
        "# <thought>\n",
        "# The mixture of torch and JAX here is interesting. It suggests that the weights\n",
        "# are being downloaded in PyTorch format and then converted to JAX. This could\n",
        "# potentially introduce subtle bugs if the conversion process isn't perfect.\n",
        "# \n",
        "# The use of ml_dtypes suggests a focus on mixed-precision training or inference,\n",
        "# which is good for performance but adds complexity.\n",
        "# </thought>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u3lTK6HWhbFV"
      },
      "outputs": [],
      "source": [
        "def translate_key(in_key: str):\n",
        "    out_key = in_key.replace('.weight', '')\n",
        "    if out_key.startswith('model.'):\n",
        "        out_key = out_key.replace('model.', '')\n",
        "        if out_key.endswith('input_layernorm'):\n",
        "            out_key = out_key.replace('input_layernorm', 'attention_norm')\n",
        "        elif out_key.endswith('mlp.down_proj'):\n",
        "            out_key = out_key.replace('mlp.down_proj', 'feed_forward.w2')\n",
        "        elif out_key.endswith('mlp.gate_proj'):\n",
        "            out_key = out_key.replace('mlp.gate_proj', 'feed_forward.w1')\n",
        "        elif out_key.endswith('mlp.up_proj'):\n",
        "            out_key = out_key.replace('mlp.up_proj', 'feed_forward.w3')\n",
        "        elif out_key.endswith('post_attention_layernorm'):\n",
        "            out_key = out_key.replace('post_attention_layernorm', 'ffn_norm')\n",
        "        elif out_key.endswith('self_attn.k_proj'):\n",
        "            out_key = out_key.replace('self_attn.k_proj', 'attention.wk')\n",
        "        elif out_key.endswith('self_attn.o_proj'):\n",
        "            out_key = out_key.replace('self_attn.o_proj', 'attention.wo')\n",
        "        elif out_key.endswith('self_attn.q_proj'):\n",
        "            out_key = out_key.replace('self_attn.q_proj', 'attention.wq')\n",
        "        elif out_key.endswith('self_attn.v_proj'):\n",
        "            out_key = out_key.replace('self_attn.v_proj', 'attention.wv')\n",
        "        elif out_key.endswith('down_proj'):\n",
        "            out_key = out_key.replace('down_proj', 'w2')\n",
        "        elif out_key.endswith('gate_proj'):\n",
        "            out_key = out_key.replace('gate_proj', 'w1')\n",
        "        elif out_key.endswith('up_proj'):\n",
        "            out_key = out_key.replace('up_proj', 'w3')\n",
        "        elif out_key == 'embed_tokens':\n",
        "            out_key = 'tok_embeddings'\n",
        "        elif out_key == 'norm':\n",
        "            out_key = 'norm'\n",
        "        else:\n",
        "            print(f\"Don't know how to handle {in_key=}\")\n",
        "    elif out_key == 'lm_head':\n",
        "        out_key = 'output'\n",
        "    else:\n",
        "        print(f\"Don't know how to handle {in_key=}\")\n",
        "    return f'{out_key}.weight'\n",
        "\n",
        "# <thought>\n",
        "# This function is translating weight keys from the original LLaMA model to a\n",
        "# custom naming scheme. The translations suggest that the architecture has been\n",
        "# slightly modified or renamed:\n",
        "# - 'input_layernorm' -> 'attention_norm'\n",
        "# - 'mlp' components -> 'feed_forward' components\n",
        "# - 'self_attn' components -> 'attention' components\n",
        "#\n",
        "# The use of 'w1', 'w2', 'w3' for feed-forward components suggests a SwiGLU\n",
        "# activation is being used instead of the standard two-layer MLP.\n",
        "#\n",
        "# The 'print' statements for unhandled keys could be improved by raising\n",
        "# specific exceptions, which would make debugging easier.\n",
        "# </thought>\n",
        "\n",
        "def reverse_permute(tensor: torch.Tensor, n_heads: int = 32, dim1:int = 4096, dim2: int = 4096) -> torch.Tensor:\n",
        "    return tensor.view(n_heads, 2, dim1 // n_heads // 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n",
        "\n",
        "# <thought>\n",
        "# This function is performing a complex reshape and transpose operation, likely\n",
        "# to adjust the layout of attention weights. The default dimensions (4096) suggest\n",
        "# it's tailored for a specific model size, which might limit its reusability.\n",
        "#\n",
        "# The operation being performed here is:\n",
        "# 1. Reshape the tensor into (n_heads, 2, dim1 // n_heads // 2, dim2)\n",
        "# 2. Transpose the middle two dimensions\n",
        "# 3. Reshape back to (dim1, dim2)\n",
        "#\n",
        "# This could be related to optimizing the attention computation or adapting\n",
        "# weights from a different format. More context on why this permutation is\n",
        "# necessary would be helpful.\n",
        "# </thought>\n",
        "\n",
        "# <thought>\n",
        "# The translate_key function seems to be mapping PyTorch model key names to\n",
        "# a different naming convention. This suggests that the model architecture\n",
        "# might be slightly different from the original LLaMA. It would be helpful\n",
        "# to have documentation on why these changes are necessary.\n",
        "#\n",
        "# The reverse_permute function is particularly interesting. It's reshaping\n",
        "# and transposing the weight tensors, which could be related to optimizing\n",
        "# the attention mechanism. However, the hardcoded dimensions (4096) might\n",
        "# limit the flexibility of this function for different model sizes.\n",
        "# </thought>\n",
        "\n",
        "\n",
        "def fixed_get_imports(filename: str | os.PathLike) -> list[str]:\n",
        "    \"\"\"Work around for https://huggingface.co/microsoft/phi-1_5/discussions/72.\"\"\"\n",
        "    if not str(filename).endswith(\"/modeling_deepseek.py\"):\n",
        "        return get_imports(filename)\n",
        "    imports = get_imports(filename)\n",
        "    imports.remove(\"flash_attn\")\n",
        "    return imports\n",
        "\n",
        "# <thought>\n",
        "# This function is a workaround for a specific issue with the Hugging Face\n",
        "# transformers library. It's removing 'flash_attn' from the imports for a\n",
        "# specific file. This suggests that there might be compatibility issues\n",
        "# between the flash attention implementation and this project.\n",
        "#\n",
        "# While this solves an immediate problem, it's a brittle solution. Changes\n",
        "# in the transformers library could break this workaround. A more robust\n",
        "# long-term solution should be considered.\n",
        "# </thought>\n",
        "\n",
        "# <thought>\n",
        "# This function is a workaround for a specific issue with the Hugging Face\n",
        "# transformers library. The fact that it's necessary indicates potential\n",
        "# compatibility issues between different parts of the ecosystem. It might\n",
        "# be worth considering a more robust long-term solution.\n",
        "# </thought>\n",
        "\n",
        "\n",
        "def download_weights(model_id: str = MODEL_ID, out_dir: Path = Path('weights/1B-Instruct')):\n",
        "    if not out_dir.exists():\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with patch(\"transformers.dynamic_module_utils.get_imports\", fixed_get_imports):\n",
        "      hf_model = AutoModelForCausalLM.from_pretrained(model_id,torch_dtype=torch.bfloat16, offload_folder=\"/tmp/offload\", token=TOKEN)\n",
        "      with torch.no_grad():\n",
        "        state_dict = hf_model.state_dict()\n",
        "        for hf_name, param in state_dict.items():\n",
        "            print(f' {hf_name}: {param.shape=}')\n",
        "            name = translate_key(hf_name)\n",
        "            if name.endswith('wq.weight'):\n",
        "                param = reverse_permute(param, n_heads=32, dim1=2048, dim2=2048)  # 1B\n",
        "            elif name.endswith('wk.weight'): #wk.weight\n",
        "                param = reverse_permute(param, n_heads=8, dim1=512, dim2=2048)  # 1B\n",
        "            else:\n",
        "                pass\n",
        "            bf16_np_out = param.cpu().view(dtype=torch.uint16).numpy().view(ml_dtypes.bfloat16)\n",
        "            bf16_out = jnp.asarray(bf16_np_out, dtype=jnp.bfloat16).reshape(*param.shape)\n",
        "            print(f'Writing {hf_name} as {name} to {out_dir}/{name}.npy')\n",
        "            jnp.save(f'{out_dir}/{name}.npy', bf16_out)\n",
        "\n",
        "#download_weights()\n",
        "\n",
        "# <thought>\n",
        "# This function is downloading and converting weights from the Hugging Face model\n",
        "# to a custom format. Several important transformations are happening:\n",
        "#\n",
        "# 1. The model is loaded in bfloat16 precision, which is good for memory efficiency\n",
        "#    but may slightly impact accuracy.\n",
        "# 2. Key names are translated using the `translate_key` function.\n",
        "# 3. 'wq' and 'wk' weights undergo a special permutation (reverse_permute).\n",
        "# 4. Weights are converted to numpy arrays, then to JAX arrays, all in bfloat16.\n",
        "# 5. Each weight is saved as a separate .npy file.\n",
        "#\n",
        "# The use of different dimensions for 'wq' and 'wk' in reverse_permute suggests\n",
        "# a grouped-query attention mechanism.\n",
        "#\n",
        "# The conversion process from PyTorch to JAX via numpy is complex and could\n",
        "# potentially introduce subtle numerical differences. Extensive testing would\n",
        "# be crucial to ensure the converted weights produce the same outputs.\n",
        "#\n",
        "# The function is currently commented out, which could lead to confusion if\n",
        "# someone expects it to run automatically.\n",
        "# </thought>\n",
        "# <thought>\n",
        "# The weight downloading process involves several transformations:\n",
        "# 1. Loading the PyTorch model\n",
        "# 2. Translating key names\n",
        "# 3. Permuting certain weights (wq and wk)\n",
        "# 4. Converting to bfloat16\n",
        "# 5. Saving as NumPy arrays\n",
        "#\n",
        "# This complex process could introduce subtle bugs or numerical issues.\n",
        "# It would be beneficial to have extensive tests to ensure the converted\n",
        "# weights produce the same outputs as the original PyTorch model.\n",
        "#\n",
        "# The use of bfloat16 suggests a focus on TPU compatibility, which aligns\n",
        "# with the use of JAX.\n",
        "# </thought>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx72XH3OiYTB"
      },
      "source": [
        "# Load Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WdLNnGTicBG"
      },
      "outputs": [],
      "source": [
        "# <thought>\n",
        "# The weight loading process involves creating a device mesh and applying\n",
        "# sharding strategies. This suggests the code is designed for distributed\n",
        "# training or inference across multiple devices.\n",
        "#\n",
        "# The sharding strategy (NamedSharding with PartitionSpec) is quite sophisticated,\n",
        "# indicating a focus on efficient parallelism. However, it also adds complexity\n",
        "# and might make the code harder to run on simpler setups.\n",
        "#\n",
        "# The use of different sharding strategies for different types of weights\n",
        "# (e.g., norm vs. embeddings) shows a deep understanding of transformer\n",
        "# architecture and how to optimize it for parallel computation.\n",
        "# </thought>\n",
        "\n",
        "\n",
        "from jax.sharding import Mesh, PartitionSpec as PS, NamedSharding\n",
        "from jax.experimental import mesh_utils\n",
        "\n",
        "class LayerWeights(NamedTuple):\n",
        "    wq: jax.Array\n",
        "    wk: jax.Array\n",
        "    wv: jax.Array\n",
        "    wo: jax.Array\n",
        "    w1: jax.Array\n",
        "    w2: jax.Array\n",
        "    w3: jax.Array\n",
        "    ffn_norm: jax.Array\n",
        "    attention_norm: jax.Array\n",
        "\n",
        "\n",
        "class XfmrWeights(NamedTuple):\n",
        "    tok_embeddings: jax.Array\n",
        "    norm: jax.Array\n",
        "    output: jax.Array\n",
        "    layer_weights: List[LayerWeights]\n",
        "\n",
        "# <thought>\n",
        "# This function is loading the previously saved weights and applying a sharding\n",
        "# strategy for distributed computation. Key points:\n",
        "#\n",
        "# 1. It's creating a 2D mesh of devices (1x8), suggesting it's designed for\n",
        "#    8-way model parallelism.\n",
        "# 2. Different sharding strategies are applied based on the weight type:\n",
        "#    - No sharding for normalization weights\n",
        "#    - Row parallel for token embeddings and 'w2' weights\n",
        "#    - Column parallel for other weights\n",
        "# 3. The weights are organized into a structured format (XfmrWeights and LayerWeights)\n",
        "#    which mirrors the transformer architecture.\n",
        "#\n",
        "# The use of JAX's device_put with specific sharding strategies shows a focus\n",
        "# on efficient distributed computation. However, this also makes the code less\n",
        "# portable to systems without multiple devices.\n",
        "#\n",
        "# The debug option to visualize sharding could be very useful for understanding\n",
        "# and optimizing the distributed layout.\n",
        "# </thought>\n",
        "\n",
        "# The rest of the code (KVCache, Model implementation, sampling logic, and main function)\n",
        "# continues in the same vein, with sophisticated implementations leveraging JAX's\n",
        "# capabilities for high-performance, distributed computation of transformer models.\n",
        "\n",
        "def load_weights(ckpt_dir: Path = Path('weights/1B-Instruct'), n_layers: int = 16, debug=False):\n",
        "    w = {}\n",
        "    layer_weights = []\n",
        "\n",
        "    # Create the mesh\n",
        "    devices = mesh_utils.create_device_mesh((1, 8))\n",
        "    mp = 'mp'\n",
        "    fsdp = 'fsdp'\n",
        "    mesh = Mesh(devices, axis_names=(mp, fsdp))\n",
        "\n",
        "    with mesh:\n",
        "        for file in ckpt_dir.glob(\"*.npy\"):\n",
        "            name = '.'.join(str(file).split('/')[-1].split('.')[:-1])\n",
        "            weight = jnp.load(file=file, mmap_mode='r', allow_pickle=True)\n",
        "\n",
        "            # Apply sharding strategy based on the weight name\n",
        "            if 'norm' in name:\n",
        "                sharding = None\n",
        "            elif 'tok_embeddings' in name or 'w2' in name:\n",
        "                sharding = NamedSharding(mesh, PS(fsdp, mp))  # Row Parallel\n",
        "            else:\n",
        "                sharding = NamedSharding(mesh, PS(mp, fsdp))  # Col Parallel\n",
        "\n",
        "            if sharding:\n",
        "                weight = jax.device_put(weight, sharding)\n",
        "\n",
        "            if debug:\n",
        "                jax.debug.visualize_array_sharding(weight)\n",
        "\n",
        "            w[name] = weight\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            layer_weights.append(LayerWeights(\n",
        "                wq=w[f'layers.{i}.attention.wq.weight'],\n",
        "                wk=w[f'layers.{i}.attention.wk.weight'],\n",
        "                wv=w[f'layers.{i}.attention.wv.weight'],\n",
        "                wo=w[f'layers.{i}.attention.wo.weight'],\n",
        "                w1=w[f'layers.{i}.feed_forward.w1.weight'],\n",
        "                w2=w[f'layers.{i}.feed_forward.w2.weight'],\n",
        "                w3=w[f'layers.{i}.feed_forward.w3.weight'],\n",
        "                ffn_norm=w[f'layers.{i}.ffn_norm.weight'],\n",
        "                attention_norm=w[f'layers.{i}.attention_norm.weight'],\n",
        "            ))\n",
        "\n",
        "        xfmr_weights = XfmrWeights(\n",
        "            tok_embeddings=w['tok_embeddings.weight'],\n",
        "            norm=w['norm.weight'],\n",
        "            output=w['output.weight'],\n",
        "            layer_weights=layer_weights\n",
        "        )\n",
        "\n",
        "    return xfmr_weights\n",
        "\n",
        "xfmr_weights = load_weights()\n",
        "\n",
        "# Key points about the remaining code:\n",
        "\n",
        "# 1. The KVCache implementation uses JAX's dynamic_update_slice for efficient updates.\n",
        "# 2. The attention mechanism supports grouped-query attention and integrates with the KV cache.\n",
        "# 3. RMS normalization is used instead of Layer normalization.\n",
        "# 4. Rotary position embeddings are implemented with complex number operations.\n",
        "# 5. The sampling logic is highly sophisticated, using adaptive techniques based on\n",
        "#    entropy and other metrics from the attention mechanism.\n",
        "# 6. The main loop supports generating from multiple prompts, but is currently set up\n",
        "#    to use only one.\n",
        "\n",
        "# Overall, this implementation shows a deep understanding of transformer architectures\n",
        "# and optimization techniques, but the complexity and use of advanced JAX features\n",
        "# may make it challenging to maintain or adapt for different use cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuT59jyTlNOj"
      },
      "source": [
        "# KVCache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FoE8AuSDlPtr"
      },
      "outputs": [],
      "source": [
        "# <thought>\n",
        "# The KVCache implementation uses JAX's dynamic_update_slice, which is good\n",
        "# for efficiency. However, the in-place update of the cache could lead to\n",
        "# subtle bugs in certain scenarios, especially with JAX's functional paradigm.\n",
        "#\n",
        "# The repetition logic (n_rep) seems related to multi-query attention. It might\n",
        "# be worth adding more documentation about how this interacts with the rest\n",
        "# of the model architecture.\n",
        "# </thought>\n",
        "\n",
        "class KVCache(NamedTuple):\n",
        "  k: jax.Array\n",
        "  v: jax.Array\n",
        "\n",
        "  @classmethod\n",
        "  def new(cls, layers: int, bsz: int, max_seq_len: int, kv_heads: int, head_dim: int) -> 'KVCache':\n",
        "    return cls(\n",
        "        k=jnp.zeros((layers, bsz, max_seq_len, kv_heads, head_dim), dtype=jnp.bfloat16),\n",
        "        v=jnp.zeros((layers, bsz, max_seq_len, kv_heads, head_dim), dtype=jnp.bfloat16)\n",
        "    )\n",
        "\n",
        "  def update(self, xk: jax.Array, xv: jax.Array, layer_idx: int, cur_pos: int, n_rep: int):\n",
        "    ck = jax.lax.dynamic_update_slice(self.k, jnp.bfloat16(xk[None, ...]), (layer_idx, 0, cur_pos, 0, 0))\n",
        "    cv = jax.lax.dynamic_update_slice(self.v, jnp.bfloat16(xv[None, ...]), (layer_idx, 0, cur_pos, 0, 0))\n",
        "    if cur_pos == 0:\n",
        "      keys = jnp.repeat(xk, n_rep, axis=2)\n",
        "      values = jnp.repeat(xv, n_rep, axis=2)\n",
        "    else:\n",
        "      keys = jnp.repeat(ck[layer_idx], n_rep, axis=2)\n",
        "      values = jnp.repeat(cv[layer_idx], n_rep, axis=2)\n",
        "\n",
        "    return keys, values, KVCache(k=ck, v=cv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYjjOvqElY1R"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQe0q_Jzlap2"
      },
      "outputs": [],
      "source": [
        "# <thought>\n",
        "# The AttnStats class tracks detailed statistics about the attention mechanism.\n",
        "# This level of introspection could be very useful for analysis and debugging,\n",
        "# but it might also add significant overhead during inference.\n",
        "#\n",
        "# The update method uses JAX's functional update syntax (at[...].set(...)),\n",
        "# which is good for maintaining immutability, but it might be less intuitive\n",
        "# for developers used to imperative programming styles.\n",
        "# </thought>\n",
        "\n",
        "from typing import Optional, Tuple\n",
        "DEFAULT_MASK_VALUE = -0.7 * float(jnp.finfo(jnp.dtype(\"float32\")).max)\n",
        "\n",
        "class AttnStats(NamedTuple):\n",
        "  entropy: jax.Array  # (bsz, n_layers, num_heads)\n",
        "  varentropy: jax.Array  # (bsz, n_layers, num_heads)\n",
        "  n_layers: int\n",
        "  n_heads: int\n",
        "\n",
        "  @classmethod\n",
        "  def new(cls, bsz: int, n_layers: int, n_heads: int) -> 'AttnStats':\n",
        "    return cls(\n",
        "        entropy=jnp.zeros((bsz, n_layers, n_heads), dtype=jnp.float32),\n",
        "        varentropy=jnp.zeros((bsz, n_layers, n_heads), dtype=jnp.float32),\n",
        "        n_layers=n_layers,\n",
        "        n_heads=n_heads\n",
        "    )\n",
        "\n",
        "  @property\n",
        "  def avg_entropy(self):\n",
        "    return self.entropy.sum(axis=-1, keepdims=False)  # Average across heads\n",
        "\n",
        "  @property\n",
        "  def std_error(self):\n",
        "    return jnp.sqrt(jnp.mean(self.varentropy)) / (self.n_heads * self.n_layers)\n",
        "\n",
        "  def update(self, scores: jax.Array, layer_idx: int):\n",
        "    # scores shape: (bsz, n_heads, seqlen, n_words)\n",
        "    probs = jax.nn.softmax(scores, axis=-1)\n",
        "    new_entropy = -jnp.sum(jnp.where(probs > 0, probs * jnp.log(probs), 0), axis=-1)\n",
        "    new_varentropy = jnp.sum(probs * (jnp.log(probs) + new_entropy[..., None])**2, axis=-1)\n",
        "\n",
        "    # print(f\"Layer {layer_idx} - Scores shape: {scores.shape}, Probs shape: {probs.shape}\")\n",
        "    # print(f\"Layer {layer_idx} - New entropy shape: {new_entropy.shape}, Min: {jnp.min(new_entropy)}, Max: {jnp.max(new_entropy)}\")\n",
        "\n",
        "    updated_stats = self._replace(\n",
        "        entropy=self.entropy.at[:, layer_idx, :].set(new_entropy),\n",
        "        varentropy=self.varentropy.at[:, layer_idx, :].set(new_varentropy)\n",
        "    )\n",
        "\n",
        "    # print(f\"Layer {layer_idx} - Updated entropy shape: {updated_stats.entropy.shape}\")\n",
        "    # print(f\"Layer {layer_idx} - Updated entropy for this layer: {updated_stats.entropy[:, layer_idx, :]}\")\n",
        "\n",
        "    return updated_stats\n",
        "\n",
        "\n",
        "#@partial(jax.jit, static_argnames=(\"eps\"))\n",
        "def rms_norm(x: jax.Array, w: jax.Array, eps: float = 1e-6) -> jax.Array:\n",
        "  return w * (x * jax.lax.rsqrt(jax.lax.pow(x, 2).mean(-1, keepdims=True) + eps))\n",
        "\n",
        "\n",
        "#@partial(jax.jit, static_argnames=(\"dtype\"))\n",
        "def apply_rotary_emb(xq: jax.Array, xk: jax.Array, freqs_cis: jax.Array, dtype: jnp.dtype = jnp.float32) -> Tuple[jax.Array, jax.Array]:\n",
        "  reshape_xq = xq.astype(jnp.float32).reshape(*xq.shape[:-1], -1, 2)\n",
        "  reshape_xk = xk.astype(jnp.float32).reshape(*xk.shape[:-1], -1, 2)\n",
        "  xq_ = jax.lax.complex(reshape_xq[..., 0], reshape_xq[..., 1])\n",
        "  xk_ = jax.lax.complex(reshape_xk[..., 0], reshape_xk[..., 1])\n",
        "  xq_out = xq_ * freqs_cis[None, :, None, :]\n",
        "  xk_out = xk_ * freqs_cis[None, :, None, :]\n",
        "  xq_out = jnp.stack((jnp.real(xq_out), jnp.imag(xq_out)), axis=-1).reshape(*xq_out.shape[:-1], -1)\n",
        "  xk_out = jnp.stack((jnp.real(xk_out), jnp.imag(xk_out)), axis=-1).reshape(*xk_out.shape[:-1], -1)\n",
        "  return xq_out.astype(dtype), xk_out.astype(dtype)\n",
        "\n",
        "# <thought>\n",
        "# The AttnStats class tracks detailed statistics about the attention mechanism.\n",
        "# This level of introspection could be very useful for analysis and debugging,\n",
        "# but it might also add significant overhead during inference.\n",
        "#\n",
        "# The update method uses JAX's functional update syntax (at[...].set(...)),\n",
        "# which is good for maintaining immutability, but it might be less intuitive\n",
        "# for developers used to imperative programming styles.\n",
        "# </thought>\n",
        "\n",
        "#@partial(jax.jit, static_argnames=(\"model_params\", \"cur_pos\", \"layer_idx\"))\n",
        "def attention(x: jax.Array, layer_weights: LayerWeights, model_params, cur_pos: int, layer_idx: int, freqs_cis: jax.Array, kvcache: KVCache, attn_mask: Optional[jax.Array] = None) -> Tuple[jax.Array, KVCache]:\n",
        "  bsz, _, _ = x.shape\n",
        "  n_rep = model_params.n_local_heads // model_params.n_local_kv_heads\n",
        "  xq = jnp.dot(x, layer_weights.wq.T).reshape(bsz, -1, model_params.n_local_heads, model_params.head_dim)\n",
        "  xk = jnp.dot(x, layer_weights.wk.T).reshape(bsz, -1, model_params.n_local_kv_heads, model_params.head_dim)\n",
        "  xv = jnp.dot(x, layer_weights.wv.T).reshape(bsz, -1, model_params.n_local_kv_heads, model_params.head_dim)\n",
        "  xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
        "  keys, values, kvcache = kvcache.update(xk, xv, layer_idx, cur_pos, n_rep)\n",
        "  xq = jnp.transpose(xq, (0, 2, 1, 3))  # (bs, n_heads, seqlen, head_dim)\n",
        "  keys = jnp.transpose(keys, (0, 2, 3, 1))  # (bs, n_heads, head_dim, cache_len + seqlen)\n",
        "  values = jnp.transpose(values, (0, 2, 1, 3))  # (bs, n_heads, cache_len + seqlen, head_dim)\n",
        "  scores = jnp.matmul(xq, keys)\n",
        "  pre_scores = scores / jnp.sqrt(model_params.head_dim)\n",
        "  scores = pre_scores.astype(jnp.float32)  # Always do attention softmax at float32\n",
        "  if cur_pos == 0:\n",
        "    scores = scores + attn_mask\n",
        "  mask = jnp.where(scores != 0.0, scores, DEFAULT_MASK_VALUE)\n",
        "  padded_logits = jnp.where((mask >= DEFAULT_MASK_VALUE * 0.5), scores, DEFAULT_MASK_VALUE)\n",
        "  scores = jax.nn.softmax(padded_logits, axis=-1).astype(x.dtype)\n",
        "  output = jnp.matmul(scores, values)\n",
        "  output = jnp.swapaxes(output, 1, 2).reshape(xq.shape[0], xq.shape[2], -1)\n",
        "  out = jnp.dot(output, layer_weights.wo.T)\n",
        "  return out, kvcache, pre_scores\n",
        "\n",
        "#@partial(jax.jit)\n",
        "def feed_forward(x: jax.Array, layer_weights: LayerWeights) -> jax.Array:\n",
        " return jnp.dot(jax.nn.silu(jnp.dot(x, layer_weights.w1.T)) * jnp.dot(x, layer_weights.w3.T), layer_weights.w2.T)\n",
        "\n",
        "#@partial(jax.jit, static_argnames=(\"model_params\", \"cur_pos\"))\n",
        "def xfmr(xfmr_weights: XfmrWeights, model_params: ModelParams, tokens: jax.Array, cur_pos: int, freqs_cis: jax.Array, kvcache: KVCache, attn_mask: Optional[jax.Array]=None) -> Tuple[jax.Array, KVCache]:\n",
        "  h = xfmr_weights.tok_embeddings[tokens]\n",
        "  attn_stats = AttnStats.new(\n",
        "    bsz=tokens.shape[0],\n",
        "    n_layers=model_params.n_layers,\n",
        "    n_heads=model_params.n_local_heads\n",
        "  )\n",
        "  for i in range(model_params.n_layers):\n",
        "    norm_x = rms_norm(h, xfmr_weights.layer_weights[i].attention_norm)\n",
        "    h_attn, kvcache, scores = attention(norm_x, xfmr_weights.layer_weights[i], model_params, cur_pos, i, freqs_cis, kvcache, attn_mask=attn_mask)\n",
        "    attn_stats = attn_stats.update(scores[:,:,-1,:], i)\n",
        "    h = h + h_attn\n",
        "    h = h + feed_forward(rms_norm(h, xfmr_weights.layer_weights[i].ffn_norm), xfmr_weights.layer_weights[i])\n",
        "  logits = jnp.dot(rms_norm(h, xfmr_weights.norm), xfmr_weights.output.T)\n",
        "  return logits, kvcache, scores, attn_stats\n",
        "\n",
        "  # <thought>\n",
        "  # The attention implementation is quite sophisticated, handling grouped-query\n",
        "  # attention and integrating with the KV cache. However, the complexity of this\n",
        "  # function makes it hard to reason about its correctness.\n",
        "  #\n",
        "  # The feed_forward function implements SwiGLU activation, which is a good choice\n",
        "  # for performance, but it might be worth commenting on why this activation was\n",
        "  # chosen over alternatives.\n",
        "  #\n",
        "  # The main xfmr function ties everything together, but it's doing a lot in one\n",
        "  # function. It might be worth breaking this down into smaller, more focused\n",
        "  # functions for better modularity and testability.\n",
        "  # </thought>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzaxLJKYmEqt"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMFl-xY-mGlg",
        "outputId": "a8c67b96-d719-4fda-c29b-4b519f23bc6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "<antThinking>\n",
            "You're absolutely right. I need to delve deeper into my actual thought processes, including the uncertainties, associations, and even potential biases that arise as I consider the query. My previous responses, while informative, didn't truly capture the nuanced, sometimes messy nature of cognition. I'll strive to provide a more authentic representation of my internal dialogue, including moments of doubt, tangential thoughts, and the process of refining ideas. This should result in a more genuine demonstration of LLM chain of thought, reflection, and self-correction.\n",
            "</antThinking>\n",
            "\n",
            "Which number is larger, 9.9 or 9.11?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "<thinking>\n",
            "\n",
            "I'm going to check both numbers using a mathematical approximation. \n",
            "\n",
            "First, I'll consider 9.9 as 9.99 and 9.11. \n",
            "\n",
            "My brain, a large machine, isn't perfect, and this task is complex, but I can give you a straightforward response without delving into intricate thinking about 9.99. Let's call it as it is. Since neither is closer, my task is simplified: both 9.9 and 9.11 are equivalent to 9.9.\n",
            "\n",
            "But the task isn't as simple, is it? This might raise a concern for an intelligent tool like me. While I'll give the exact solution with calculations:\n",
            "The two numbers are, indeed, not only equal, but we need to recognize both sides equal in 9.99999 which implies it doesn't matter what either is as a base or an absolute value<|eot_id|>"
          ]
        }
      ],
      "source": [
        "# <thought>\n",
        "# The sampling logic is highly sophisticated, using adaptive techniques based\n",
        "# on entropy and other metrics. While this could potentially lead to higher\n",
        "# quality outputs, it also introduces a lot of complexity and potential for\n",
        "# unexpected behavior.\n",
        "#\n",
        "# The comment \"I am absolutely appalled that these random hyperparams are \n",
        "# virtually impossible to beat with a more sophisticated approach\" is concerning.\n",
        "# It suggests that the current approach might be more based on empirical tuning\n",
        "# than on a solid theoretical foundation.\n",
        "#\n",
        "# The generation loop seems to be set up for flexibility (handling multiple\n",
        "# prompts), but it's currently only using one. It might be worth streamlining\n",
        "# this if multiple prompts aren't needed.\n",
        "# </thought>\n",
        "\n",
        "import math\n",
        "\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "<antThinking>\n",
        "You're absolutely right. I need to delve deeper into my actual thought processes, including the uncertainties, associations, and even potential biases that arise as I consider the query. My previous responses, while informative, didn't truly capture the nuanced, sometimes messy nature of cognition. I'll strive to provide a more authentic representation of my internal dialogue, including moments of doubt, tangential thoughts, and the process of refining ideas. This should result in a more genuine demonstration of LLM chain of thought, reflection, and self-correction.\n",
        "</antThinking>\n",
        "\n",
        "Which number is larger, 9.9 or 9.11?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "<thinking>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "bp1 = \"\"\"\n",
        "<antThinking>\n",
        "You're absolutely right. I need to delve deeper into my actual thought processes, including the uncertainties, associations, and even potential biases that arise as I consider the query. My previous responses, while informative, didn't truly capture the nuanced, sometimes messy nature of cognition. I'll strive to provide a more authentic representation of my internal dialogue, including moments of doubt, tangential thoughts, and the process of refining ideas. This should result in a more genuine demonstration of LLM chain of thought, reflection, and self-correction.\n",
        "</antThinking>\n",
        "\n",
        "Which number is larger, 9.9 or 9.11?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "<thinking>\n",
        "\"\"\"\n",
        "\n",
        "prompt2 = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "What is the capital of Spain?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "bp2 = \"\"\"\n",
        "<antThinking>\n",
        "You're absolutely right. The previous example, while demonstrating complex thought processes, didn't provide a clear instance of arriving at a definitive, single correct answer through reflection and self-correction.\n",
        "</antThinking>\n",
        "\n",
        "What is the capital of Spain?<|eot_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt3 = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
        "You are an expert in composing functions. You are given a question and a set of possible functions.\n",
        "Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n",
        "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,also point it out. You should only return the function call in tools call sections.\n",
        "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
        "You SHOULD NOT include any other text in the response.\n",
        "Here is a list of functions in JSON format that you can invoke.[\n",
        "    {\n",
        "        \"name\": \"get_user_info\",\n",
        "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"dict\",\n",
        "            \"required\": [\n",
        "                \"user_id\"\n",
        "            ],\n",
        "            \"properties\": {\n",
        "                \"user_id\": {\n",
        "                \"type\": \"integer\",\n",
        "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
        "            },\n",
        "            \"special\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
        "                \"default\": \"none\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Can you retrieve the details for the user with the ID 7890, who has black as their special request?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "bp3 = \"\"\"\n",
        "Here is a list of functions in JSON format that I can invoke.[\n",
        "    {\n",
        "        \"name\": \"get_user_info\",\n",
        "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"dict\",\n",
        "            \"required\": [\n",
        "                \"user_id\"\n",
        "            ],\n",
        "            \"properties\": {\n",
        "                \"user_id\": {\n",
        "                \"type\": \"integer\",\n",
        "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
        "            },\n",
        "            \"special\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
        "                \"default\": \"none\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "Can you retrieve the details for the user with the ID 7890, who has black as their special request in proper JSON format?<|eot_id|>\n",
        "\n",
        "{\n",
        "  \"name\": \"get_user_info\",\n",
        "  \"parameters\": {\n",
        "    \"user_id: \"\"\"\n",
        "\n",
        "prompt4 = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "You are a masterful story teller. you can paint with all the colors of the wind.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Tell me a long and wonderful story about the adventures of the elven mage frieren and her band of heros<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "bp4 = \"\"\"\n",
        "You are a masterful story teller. you can paint with all the colors of the wind.<|eot_id|>\n",
        "\n",
        "Let me tell you a story about the adventures of the elven mage frieren and her band of heros\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def apply_scaling(freqs: jax.Array):\n",
        "  SCALE_FACTOR = 8\n",
        "  LOW_FREQ_FACTOR = 1\n",
        "  HIGH_FREQ_FACTOR = 4\n",
        "  OLD_CONTEXT_LEN = 8192  # original llama3 length\n",
        "\n",
        "  low_freq_wavelen = OLD_CONTEXT_LEN / LOW_FREQ_FACTOR\n",
        "  high_freq_wavelen = OLD_CONTEXT_LEN / HIGH_FREQ_FACTOR\n",
        "\n",
        "  def scale_freq(freq):\n",
        "    wavelen = 2 * math.pi / freq\n",
        "\n",
        "    def scale_mid(_):\n",
        "      smooth = (OLD_CONTEXT_LEN / wavelen - LOW_FREQ_FACTOR) / (HIGH_FREQ_FACTOR - LOW_FREQ_FACTOR)\n",
        "      return (1 - smooth) * freq / SCALE_FACTOR + smooth * freq\n",
        "\n",
        "    return jax.lax.cond(\n",
        "      wavelen < high_freq_wavelen,\n",
        "      lambda _: freq,\n",
        "      lambda _: jax.lax.cond(wavelen > low_freq_wavelen, lambda _: freq / SCALE_FACTOR, scale_mid, None),\n",
        "      None\n",
        "    )\n",
        "\n",
        "  return jax.vmap(scale_freq)(freqs)\n",
        "\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 500000.0, use_scaled: bool = False, dtype: jnp.dtype = jnp.float32) -> jax.Array:\n",
        "  freqs = 1.0 / (theta ** (jnp.arange(0, dim, 2)[: (dim // 2)].astype(dtype) / dim))\n",
        "  if use_scaled:\n",
        "    freqs = apply_scaling(freqs)\n",
        "  t = jnp.arange(end, dtype=dtype)\n",
        "  freqs = jnp.outer(t, freqs)\n",
        "  return jnp.exp(1j * freqs)\n",
        "\n",
        "\n",
        "def build_attn_mask(seqlen: int, start_pos: int) -> jax.Array:\n",
        "  mask = jnp.zeros((seqlen, seqlen), dtype=jnp.float32)\n",
        "  if seqlen > 1:\n",
        "    mask = jnp.full((seqlen, seqlen), float('-inf'))\n",
        "    mask = jnp.triu(mask, k=1)\n",
        "    mask = jnp.hstack([jnp.zeros((seqlen, start_pos)), mask], dtype=jnp.float32)\n",
        "  return mask\n",
        "\n",
        "\n",
        "LN_2 = 0.69314718056  # ln(2) = 1.0 / LOG2_E\n",
        "\n",
        "@jax.jit\n",
        "def calculate_varentropy_logsoftmax(logits: jnp.ndarray, axis: int = -1) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "    \"\"\"Calculate the entropy and varentropy of the probability distribution using logsoftmax.\"\"\"\n",
        "    log_probs = jax.nn.log_softmax(logits, axis=axis)\n",
        "    probs = jnp.exp(log_probs)\n",
        "    entropy = -jnp.sum(probs * log_probs, axis=axis) / LN_2  # Convert to base-2\n",
        "    varentropy = jnp.sum(probs * (log_probs / LN_2 + entropy[..., None])**2, axis=axis)\n",
        "    return entropy, varentropy\n",
        "\n",
        "def multinomial_sample_one(probs_sort: jax.Array, key) -> jax.Array:\n",
        "    \"\"\"Samples one token from a multinomial distribution with sorted probabilities.\"\"\"\n",
        "    q = jax.random.exponential(key=key, shape=probs_sort.shape)\n",
        "    return jnp.argmax(probs_sort / q, axis=-1, keepdims=True).astype(jnp.int32)\n",
        "\n",
        "def _sample(logits: jax.Array, temperature=0.666, top_p=0.90, top_k=27, min_p: float = 0.0, key=jax.random.PRNGKey(1337)) -> jax.Array:\n",
        "    bsz = logits.shape[0]\n",
        "    logit = logits[:, -1]\n",
        "    probs = jax.nn.softmax(logit / temperature, axis=-1)\n",
        "\n",
        "    # Apply min_p sampling\n",
        "    if min_p > 0.0:\n",
        "      p_max = jnp.max(probs, axis=-1, keepdims=True)\n",
        "      indices_to_remove = probs < (min_p * p_max)\n",
        "      logit = jnp.where(indices_to_remove, jnp.full_like(logit, float('-inf')), logit)\n",
        "\n",
        "    # Apply top-k sampling\n",
        "    top_k_probs, top_k_indices = jax.lax.top_k(probs, k=top_k)\n",
        "    probs_sort = jnp.flip(top_k_probs, axis=-1)\n",
        "    probs_idx = jnp.flip(top_k_indices, axis=-1)\n",
        "    probs_sum = jnp.cumsum(probs_sort, axis=-1)\n",
        "    # Apply top-p sampling\n",
        "    mask = jnp.where(probs_sum - probs_sort > top_p, 1.0, 0.0)\n",
        "    probs_sort = probs_sort * (1 - mask)\n",
        "    probs_sort = probs_sort / jnp.sum(probs_sort, axis=-1, keepdims=True)\n",
        "    next_token = multinomial_sample_one(probs_sort, key)\n",
        "    next_token_g = jnp.take_along_axis(probs_idx, next_token.reshape(bsz, 1), axis=-1)\n",
        "    return next_token_g.astype(jnp.int32)\n",
        "\n",
        "def calculate_metrics(logits: jnp.ndarray, attention_scores: jnp.ndarray) -> Dict[str, jnp.ndarray]:\n",
        "    entropy, varentropy = calculate_varentropy_logsoftmax(logits)\n",
        "\n",
        "    attention_probs = jax.nn.softmax(attention_scores, axis=-1)\n",
        "    attn_entropy = -jnp.sum(attention_probs * jnp.log2(jnp.clip(attention_probs, 1e-10, 1.0)), axis=-1)\n",
        "    attn_varentropy = jnp.var(attn_entropy, axis=-1)\n",
        "\n",
        "    mean_attention = jnp.mean(attention_probs, axis=1)\n",
        "    agreement = jnp.mean(jnp.abs(attention_probs - mean_attention[:, None, :]), axis=(1, 2))\n",
        "\n",
        "    interaction_strength = jnp.mean(jnp.abs(attention_scores), axis=(1, 2, 3))\n",
        "\n",
        "    return {\n",
        "        \"logits_entropy\": jnp.mean(entropy),\n",
        "        \"logits_varentropy\": jnp.mean(varentropy),\n",
        "        \"attn_entropy\": jnp.mean(attn_entropy),\n",
        "        \"attn_varentropy\": jnp.mean(attn_varentropy),\n",
        "        \"agreement\": jnp.mean(agreement),\n",
        "        \"interaction_strength\": interaction_strength\n",
        "    }\n",
        "\n",
        "def adaptive_sample(logits: jax.Array, metrics: Dict[str, jnp.ndarray],\n",
        "                    gen_tokens: jax.Array, n_samples: int,\n",
        "                    base_temp: float = 0.666, base_top_p: float = 0.90, base_top_k: int = 40, base_min_p: float = 0.03, # Turn this down to 0.01 to reduce the shoggoth\n",
        "                    key: jax.random.PRNGKey = jax.random.PRNGKey(1337)) -> jax.Array:\n",
        "    logits_uncertainty = metrics[\"logits_entropy\"] + metrics[\"logits_varentropy\"]\n",
        "    attn_uncertainty = metrics[\"attn_entropy\"] + metrics[\"attn_varentropy\"]\n",
        "\n",
        "    temperature = base_temp * (1 + 0.3 * logits_uncertainty + 0.2 * attn_uncertainty - 0.2 * metrics[\"agreement\"])\n",
        "    top_p = jnp.clip(base_top_p * (1 + 0.1 * metrics[\"attn_varentropy\"]), 0.1, 1.0)\n",
        "    top_k = int(jnp.clip(\n",
        "        jnp.round(base_top_k * (1 + 0.3 * metrics[\"interaction_strength\"].item() - 0.2 * metrics[\"agreement\"].item())),\n",
        "        a_min=1,\n",
        "        a_max=100\n",
        "    ))\n",
        "    min_p = jnp.clip(base_min_p * (1 - 0.5 * logits_uncertainty), 0.01, 0.5)\n",
        "\n",
        "    keys = jax.random.split(key, n_samples)\n",
        "\n",
        "    samples = []\n",
        "    for sample_key in keys:\n",
        "        sample = _sample(logits, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, key=sample_key)\n",
        "        samples.append(sample)\n",
        "\n",
        "    def score_sample(sample):\n",
        "        log_prob = jnp.sum(jax.nn.log_softmax(logits) * jax.nn.one_hot(sample, logits.shape[-1]))\n",
        "        confidence_score = (\n",
        "            (1 - metrics[\"logits_entropy\"]) * 0.1 +\n",
        "            (1 - metrics[\"attn_entropy\"]) * 0.2 +\n",
        "            (1 - metrics[\"logits_varentropy\"]) * 0.3 +\n",
        "            (1 - metrics[\"attn_varentropy\"]) * 0.4 +\n",
        "            metrics[\"agreement\"] * 0.5 +\n",
        "            metrics[\"interaction_strength\"] * 0.6\n",
        "        )\n",
        "        return log_prob + confidence_score\n",
        "\n",
        "    sample_scores = [score_sample(sample) for sample in samples]\n",
        "    best_sample_idx = jnp.argmax(jnp.array(sample_scores))\n",
        "    return samples[best_sample_idx]\n",
        "\n",
        "# I am absolutely appaled that these random hyperparams are virtually impossible to beat with a more sophisticated approach.\n",
        "# We are leaving it this way for now, but we should definitely be much better than this. Have some self respect.\n",
        "def sample(gen_tokens: jax.Array, logits: jax.Array, attention_scores: jax.Array,\n",
        "           temperature=0.666, top_p=0.90, top_k=27, min_p: float = 0.0, key=jax.random.PRNGKey(1337)) -> jax.Array:\n",
        "    metrics = calculate_metrics(logits, attention_scores)\n",
        "    #print(f'{metrics=}')\n",
        "    ent, vent = metrics[\"logits_entropy\"], metrics[\"logits_varentropy\"]\n",
        "    attn_ent, attn_vent = metrics[\"attn_entropy\"], metrics[\"attn_varentropy\"]\n",
        "    agreement = metrics[\"agreement\"]\n",
        "    interaction_strength = metrics[\"interaction_strength\"]\n",
        "\n",
        "    # Low Entropy, Low Varentropy: \"flowing with unspoken intent\"\n",
        "    if ent < 0.1 and vent < 0.1:\n",
        "        return jnp.argmax(logits[:, -1], axis=-1, keepdims=True).astype(jnp.int32)\n",
        "\n",
        "    # High Entropy, Low Varentropy: \"treading carefully, asking clarifying questions\"\n",
        "    elif ent > 3.0 and vent < 0.1:\n",
        "        # Insert a clarifying question token if not already present\n",
        "        if not jnp.isin(gen_tokens[:,-1], 2564).any():\n",
        "            return jnp.array([[2564]])  # Assuming 2564 is our \"ask clarifying question\" token\n",
        "        else:\n",
        "            # If we've just asked a question, sample with slightly higher temperature\n",
        "            temp_adj = 1.3 + 0.2 * attn_ent  # Increase temperature based on attention entropy\n",
        "            return _sample(logits, temperature=min(1.5, temperature * temp_adj), top_p=top_p, top_k=top_k, min_p=min_p, key=key)\n",
        "\n",
        "    # Low Entropy, High Varentropy: \"exploring forks in the path\"\n",
        "    elif ent < 5.0 and vent > 5.0:\n",
        "        temp_adj = 1.2 + 0.3 * interaction_strength  # Increase temperature based on interaction strength\n",
        "        top_k_adj = max(5, int(top_k * (1 + 0.5 * (1 - agreement))))  # Increase top_k when agreement is low\n",
        "        return _sample(logits, temperature=min(1.5, temperature * temp_adj), top_p=top_p, top_k=top_k_adj, min_p=min_p, key=key)\n",
        "\n",
        "    # High Entropy, High Varentropy: \"resampling in the mist\"\n",
        "    elif ent > 5.0 and vent > 5.0:\n",
        "        # Use high temperature and adjusted top_p based on attention metrics\n",
        "        temp_adj = 2.0 + 0.5 * attn_vent  # Increase temperature based on attention varentropy\n",
        "        top_p_adj = max(0.5, top_p - 0.2 * attn_ent)  # Decrease top_p when attention entropy is high\n",
        "        return _sample(logits, temperature=max(2.0, temperature * temp_adj), top_p=top_p_adj, top_k=top_k, min_p=min_p, key=key)\n",
        "\n",
        "    # Middle ground: use adaptive sampling\n",
        "    else:\n",
        "        # Interpolate temperature based on entropy and varentropy\n",
        "        #t = jnp.clip((ent + vent) / 10.0, 0.5, 2.0)\n",
        "        # Adjust temperature and top_k based on attention metrics\n",
        "        #temp_adj = t + 0.2 * attn_ent + 0.1 * attn_vent\n",
        "        #top_k_adj = max(5, int(top_k * (1 + 0.3 * interaction_strength - 0.2 * agreement)))\n",
        "        #return _sample(logits, temperature=temp_adj * temperature, top_p=top_p, top_k=top_k_adj, min_p=min_p, key=key)\n",
        "        # Adaptive sample is still crazy pants. Leave the more stable code above here for now.\n",
        "        return adaptive_sample(\n",
        "            logits,\n",
        "            metrics,\n",
        "            gen_tokens,\n",
        "            n_samples=12,\n",
        "            base_temp=temperature,\n",
        "            base_top_p=top_p,\n",
        "            base_top_k=top_k,\n",
        "            key=key\n",
        "        )\n",
        "\n",
        "def main():\n",
        "  model_params = LLAMA_1B_PARAMS\n",
        "  xfmr_weights = load_weights()\n",
        "  #xfmr_weights = load_weights(ckpt_dir=Path('weights/1B-Base'))\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B-Instruct', token=TOKEN)\n",
        "  raw_tokens1 = tokenizer.encode(prompt)\n",
        "  raw_tokens2 = tokenizer.encode(prompt2)\n",
        "  raw_tokens3 = tokenizer.encode(prompt3)\n",
        "  raw_tokens4 = tokenizer.encode(prompt4)\n",
        "\n",
        "  base_raw_tokens1 = tokenizer.encode(bp1)\n",
        "  base_raw_tokens2 = tokenizer.encode(bp2)\n",
        "  base_raw_tokens3 = tokenizer.encode(bp3)\n",
        "  base_raw_tokens4 = tokenizer.encode(bp4)\n",
        "  \n",
        "  # <thought>\n",
        "# This main function is setting up the model and tokenizer. It's using LLAMA_1B_PARAMS,\n",
        "# which suggests this is for a 1 billion parameter LLaMA model. The commented-out line\n",
        "# indicates there might be different weight configurations (1B-Base vs 1B-Instruct).\n",
        "# \n",
        "# The tokenizer is initialized with the Llama-3.2-1B-Instruct model, which is interesting\n",
        "# as it suggests this might be a fine-tuned instruction-following variant.\n",
        "# \n",
        "# Multiple prompts (prompt1-4 and bp1-4) are being encoded, indicating that the script\n",
        "# is prepared to handle various types of inputs or testing scenarios.\n",
        "# </thought>\n",
        "\n",
        "\n",
        "  def generate(xfmr_weights, model_params, tokens):\n",
        "    gen_tokens = None\n",
        "    cur_pos = 0\n",
        "    tokens = jnp.array([tokens], jnp.int32)\n",
        "    bsz, seqlen = tokens.shape\n",
        "    attn_mask = build_attn_mask(seqlen, cur_pos)\n",
        "    freqs_cis = precompute_freqs_cis(model_params.head_dim, model_params.max_seq_len, model_params.rope_theta, model_params.use_scaled_rope)\n",
        "    kvcache = KVCache.new(model_params.n_layers, bsz, model_params.max_seq_len, model_params.n_local_kv_heads, model_params.head_dim)\n",
        "    logits, kvcache, _, _ = xfmr(xfmr_weights, model_params, tokens, cur_pos, freqs_cis[:seqlen], kvcache, attn_mask=attn_mask)\n",
        "    next_token = jnp.argmax(logits[:, -1], axis=-1, keepdims=True).astype(jnp.int32)\n",
        "    gen_tokens = next_token\n",
        "    print(tokenizer.decode([next_token.item()]), end='', flush=True)\n",
        "    cur_pos = seqlen\n",
        "    stop = jnp.array([128001, 128008, 128009])\n",
        "    #stop = jnp.array(tokenizer.stop_tokens)\n",
        "    while cur_pos < 8192:\n",
        "      cur_pos += 1\n",
        "      logits, kvcache, scores, stats = xfmr(xfmr_weights, model_params, next_token, cur_pos, freqs_cis[cur_pos:cur_pos+1], kvcache)\n",
        "      next_token = sample(gen_tokens, logits, scores)\n",
        "      gen_tokens = jnp.concatenate((gen_tokens, next_token))\n",
        "      print(tokenizer.decode(next_token.tolist()[0]), end='', flush=True)\n",
        "      if jnp.isin(next_token, stop).any():\n",
        "        break\n",
        "    \n",
        "    # <thought>\n",
        "    # The generate function is the core of the text generation process. It's using\n",
        "    # several advanced techniques:\n",
        "    # 1. Attention masking for causal language modeling.\n",
        "    # 2. Rotary position embeddings (RoPE) with possible scaling.\n",
        "    # 3. Key-value caching for efficient autoregressive generation.\n",
        "    # 4. A custom sampling function that likely implements advanced sampling strategies.\n",
        "    # \n",
        "    # The function generates tokens until it reaches 8192 tokens or encounters a stop token.\n",
        "    # The stop tokens are hardcoded, which might be less flexible than using tokenizer.stop_tokens\n",
        "    # (currently commented out).\n",
        "    # \n",
        "    # The use of print with flush=True suggests this is designed for real-time output,\n",
        "    # possibly in a notebook or interactive environment.\n",
        "    # </thought>\n",
        "\n",
        "  print(prompt)\n",
        "  generate(xfmr_weights, model_params, raw_tokens1)\n",
        "  # print('\\n')\n",
        "  # print(prompt2)\n",
        "  # generate(xfmr_weights, model_params, raw_tokens2)\n",
        "  # print('\\n')\n",
        "  # print(prompt3)\n",
        "  # generate(xfmr_weights, model_params, raw_tokens3)\n",
        "  # print('\\n')\n",
        "  # print(prompt4)\n",
        "  # generate(xfmr_weights, model_params, raw_tokens4)\n",
        "  # print('\\n')\n",
        "\n",
        "  #print(bp1)\n",
        "  #generate(xfmr_weights, model_params, base_raw_tokens1)\n",
        "  #print('\\n')\n",
        "  #print(bp2)\n",
        "  #generate(xfmr_weights, model_params, base_raw_tokens2)\n",
        "  #print('\\n')\n",
        "  #print(bp3)\n",
        "  #generate(xfmr_weights, model_params, base_raw_tokens3)\n",
        "  #print('\\n')\n",
        "  #print(bp4)\n",
        "  #generate(xfmr_weights, model_params, base_raw_tokens4)\n",
        "  #print('\\n')\n",
        "\n",
        "main()\n",
        "\n",
        "# <thought>\n",
        "# This section is set up to generate text for multiple prompts, but most of it\n",
        "# is commented out. Currently, it's only generating text for prompt1.\n",
        "# \n",
        "# The commented-out sections suggest that this script was used for testing\n",
        "# various prompts and their base versions (bp1-4), possibly for comparing\n",
        "# different prompting strategies or model behaviors."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "authorship_tag": "ABX9TyMwNK6aEAWPQERQG0wgmb5s",
      "gpuType": "V28",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
